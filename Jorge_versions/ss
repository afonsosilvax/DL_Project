{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Models for Imbalanced Classification in PySpark\n",
    "\n",
    "This notebook implements multiple classification models using PySpark with techniques specifically designed for handling class imbalance.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Configuration and Setup](#1.-Configuration-and-Setup)\n",
    "2. [Importing Libraries](#2.-Importing-Libraries)\n",
    "3. [Loading and Exploring Data](#3.-Loading-and-Exploring-Data)\n",
    "4. [Class Imbalance Handling](#4.-Class-Imbalance-Handling)\n",
    "5. [Helper Functions](#5.-Helper-Functions)\n",
    "    - [5.1 Random Search Function](#5.1-Random-Search-Function)\n",
    "    - [5.2 Evaluation Functions](#5.2-Evaluation-Functions)\n",
    "6. [Model Training](#6.-Model-Training)\n",
    "    - [6.1 Logistic Regression](#6.1-Logistic-Regression)\n",
    "    - [6.2 Random Forest](#6.2-Random-Forest)\n",
    "    - [6.3 Gradient Boosted Trees](#6.3-Gradient-Boosted-Trees-(GBDT))\n",
    "    - [6.4 Multilayer Perceptron](#6.4-Multilayer-Perceptron-(MLP))\n",
    "7. [Model Comparison](#7.-Model-Comparison)\n",
    "8. [Threshold Optimization](#8.-Threshold-Optimization)\n",
    "9. [Test Set Evaluation](#9.-Test-Set-Evaluation-with-Best-Model)\n",
    "10. [Conclusion](#10.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection configuration - Set to True to run, False to skip\n",
    "RUN_MODELS = {\n",
    "    'logistic_regression': True,\n",
    "    'random_forest': True,\n",
    "    'gbdt': True,\n",
    "    'mlp': True\n",
    "}\n",
    "\n",
    "# Class imbalance handling configuration\n",
    "IMBALANCE_CONFIG = {\n",
    "    # Balance method: None, 'class_weight', 'undersample', 'oversample', or 'both'\n",
    "    'method': 'both',\n",
    "    \n",
    "    # Undersampling configuration (if method = 'undersample' or 'both')\n",
    "    'undersample_ratio': 0.4,  # Higher = keep more majority class samples\n",
    "    \n",
    "    # Oversampling configuration (if method = 'oversample' or 'both')\n",
    "    'oversample_ratio': 0.7,  # Target ratio of minority to majority class\n",
    "    \n",
    "    # Apply threshold adjustment after model training\n",
    "    'adjust_threshold': True,\n",
    "    \n",
    "    # Enable class weights (for models that support it)\n",
    "    'use_class_weights': True\n",
    "}\n",
    "\n",
    "# Random search configuration\n",
    "RANDOM_SEARCH_CONFIG = {\n",
    "    'num_folds': 3,      # Number of folds for cross-validation\n",
    "    'parallelism': 1     # Reduced to 1 to avoid resource issues\n",
    "}\n",
    "\n",
    "# File paths configuration\n",
    "DATA_PATHS = {\n",
    "    'train': \"dbfs:/FileStore/tables/train_df-2.csv\",\n",
    "    'val': \"dbfs:/FileStore/tables/val_df.csv\",\n",
    "    'test': \"dbfs:/FileStore/tables/test_df-2.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from math import ceil\n",
    "import time\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, when, lit, count, lag, expr, udf, rand\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "\n",
    "# ML imports for classification\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Scikit-learn imports for metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ML Models Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Exploring Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths):\n",
    "    \"\"\"Load and preprocess data from specified file paths.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (dict): Dictionary with keys 'train', 'val', 'test' and file path values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Preprocessed train, validation, and test data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    train_data = spark.read.csv(file_paths['train'], header=True, inferSchema=True)\n",
    "    val_data = spark.read.csv(file_paths['val'], header=True, inferSchema=True)\n",
    "    test_data = spark.read.csv(file_paths['test'], header=True, inferSchema=True)\n",
    "    \n",
    "    # Select feature columns (all except 'label', 'time', and 'file')\n",
    "    feature_cols = [col for col in train_data.columns if col not in ['label', 'time', 'file']]\n",
    "    \n",
    "    # Assemble features into a single vector column\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_data).select(\"features\", \"label\")\n",
    "    val_data = assembler.transform(val_data).select(\"features\", \"label\")\n",
    "    test_data = assembler.transform(test_data).select(\"features\", \"label\")\n",
    "    \n",
    "    print(f\"Data loaded and preprocessed:\")\n",
    "    print(f\"  - Training samples: {train_data.count()}\")\n",
    "    print(f\"  - Validation samples: {val_data.count()}\")\n",
    "    print(f\"  - Test samples: {test_data.count()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def analyze_class_distribution(dataframe, title=\"Class Distribution\"):\n",
    "    \"\"\"Analyze and visualize class distribution in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: PySpark DataFrame with 'label' column\n",
    "        title: Title for the analysis\n",
    "    \n",
    "    Returns:\n",
    "        dict: Class distribution statistics\n",
    "    \"\"\"\n",
    "    # Count samples by class\n",
    "    class_counts = dataframe.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_count = dataframe.count()\n",
    "    class_stats = {}\n",
    "    \n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"Class\\tCount\\tPercentage\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    classes = []\n",
    "    counts = []\n",
    "    percentages = []\n",
    "    \n",
    "    for row in class_counts:\n",
    "        class_label = row[\"label\"]\n",
    "        count = row[\"count\"]\n",
    "        percentage = (count / total_count) * 100\n",
    "        \n",
    "        classes.append(class_label)\n",
    "        counts.append(count)\n",
    "        percentages.append(percentage)\n",
    "        \n",
    "        class_stats[class_label] = {\"count\": count, \"percentage\": percentage}\n",
    "        print(f\"{class_label}\\t{count}\\t{percentage:.2f}%\")\n",
    "    \n",
    "    # Calculate imbalance ratio (majority / minority)\n",
    "    majority_count = max(counts)\n",
    "    minority_count = min(counts)\n",
    "    imbalance_ratio = majority_count / minority_count\n",
    "    print(f\"\\nImbalance Ratio (majority/minority): {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, counts)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add count labels on top of bars\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(classes[i], count + (0.01 * majority_count), f\"{count}\\n({percentages[i]:.2f}%)\", \n",
    "                 ha='center', va='bottom')\n",
    "    \n",
    "    plt.xticks(classes)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return class_stats\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_data, val_data, test_data = load_and_preprocess_data(DATA_PATHS)\n",
    "\n",
    "# Display a few samples\n",
    "print(\"\\nSample of training data:\")\n",
    "train_data.show(3)\n",
    "\n",
    "# Analyze class distribution\n",
    "train_class_stats = analyze_class_distribution(train_data, \"Training Data Class Distribution\")\n",
    "val_class_stats = analyze_class_distribution(val_data, \"Validation Data Class Distribution\")\n",
    "test_class_stats = analyze_class_distribution(test_data, \"Test Data Class Distribution\")\n",
    "\n",
    "# Count classes\n",
    "num_classes = len(train_class_stats)\n",
    "print(f\"\\nNumber of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class Imbalance Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_class_imbalance(train_data, class_stats, config):\n",
    "    \"\"\"Apply class imbalance handling techniques to the training data.\n",
    "    \n",
    "    Args:\n",
    "        train_data: PySpark DataFrame with 'features' and 'label' columns\n",
    "        class_stats: Dictionary with class distribution statistics\n",
    "        config: Configuration dictionary for imbalance handling\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Processed training data\n",
    "        dict: Class weights dictionary (if enabled)\n",
    "    \"\"\"\n",
    "    method = config.get('method', None)\n",
    "    processed_data = train_data\n",
    "    \n",
    "    # Calculate class weights based on inverse frequency\n",
    "    total_samples = sum(stats[\"count\"] for stats in class_stats.values())\n",
    "    class_weights = {}\n",
    "    \n",
    "    for class_label, stats in class_stats.items():\n",
    "        weight = total_samples / (num_classes * stats[\"count\"])\n",
    "        class_weights[class_label] = weight\n",
    "    \n",
    "    print(\"\\nClass weights:\")\n",
    "    for class_label, weight in class_weights.items():\n",
    "        print(f\"Class {class_label}: {weight:.4f}\")\n",
    "    \n",
    "    # Apply class weight column if enabled\n",
    "    if config.get('use_class_weights', False):\n",
    "        print(\"\\nApplying class weights as a column in the dataset...\")\n",
    "        # Create a weight column based on class\n",
    "        weight_expr = None\n",
    "        for label, weight in class_weights.items():\n",
    "            if weight_expr is None:\n",
    "                weight_expr = when(col(\"label\") == label, lit(weight))\n",
    "            else:\n",
    "                weight_expr = weight_expr.when(col(\"label\") == label, lit(weight))\n",
    "        \n",
    "        processed_data = processed_data.withColumn(\"weight\", weight_expr)\n",
    "    \n",
    "    # Apply resampling methods\n",
    "    if method in ['undersample', 'both']:\n",
    "        print(\"\\nApplying undersampling to majority class...\")\n",
    "        \n",
    "        # Find majority and minority classes\n",
    "        class_counts = [(label, stats[\"count\"]) for label, stats in class_stats.items()]\n",
    "        class_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "        majority_class = class_counts[0][0]\n",
    "        majority_count = class_counts[0][1]\n",
    "        \n",
    "        # Determine fraction to sample from majority class\n",
    "        minority_counts = [count for label, count in class_counts[1:]]  \n",
    "        avg_minority_count = sum(minority_counts) / len(minority_counts)\n",
    "        undersampling_fraction = min(1.0, (avg_minority_count / majority_count) * config.get('undersample_ratio', 1.0))\n",
    "        \n",
    "        # Apply undersampling\n",
    "        majority_samples = processed_data.filter(col(\"label\") == majority_class).sample(False, undersampling_fraction)\n",
    "        other_samples = processed_data.filter(col(\"label\") != majority_class)\n",
    "        processed_data = majority_samples.union(other_samples)\n",
    "        \n",
    "        print(f\"Undersampled majority class (class {majority_class}) with fraction: {undersampling_fraction:.4f}\")\n",
    "        \n",
    "    if method in ['oversample', 'both']:\n",
    "        print(\"\\nApplying oversampling to minority classes...\")\n",
    "        \n",
    "        # Find counts after potential undersampling\n",
    "        current_counts = processed_data.groupBy(\"label\").count().collect()\n",
    "        class_counts = {row[\"label\"]: row[\"count\"] for row in current_counts}\n",
    "        \n",
    "        # Sort classes by count\n",
    "        sorted_classes = sorted(class_counts.items(), key=lambda x: x[1])\n",
    "        minority_class = sorted_classes[0][0]\n",
    "        minority_count = sorted_classes[0][1]\n",
    "        majority_class = sorted_classes[-1][0]\n",
    "        majority_count = sorted_classes[-1][1]\n",
    "        \n",
    "        # Temporary result to build up with oversampled data\n",
    "        result = processed_data\n",
    "        \n",
    "        # Oversample all classes except the majority\n",
    "        target_count = int(majority_count * config.get('oversample_ratio', 0.6))\n",
    "        \n",
    "        for class_label, count in sorted_classes[:-1]:  # Skip the majority class\n",
    "            if count < target_count:\n",
    "                # Calculate number of times to repeat (at least 1)\n",
    "                multiplier = max(1, int(target_count / count))\n",
    "                remainder_fraction = (target_count % count) / count\n",
    "                \n",
    "                # Get the minority samples\n",
    "                minority_samples = processed_data.filter(col(\"label\") == class_label)\n",
    "                \n",
    "                # Apply oversampling by repeating the data\n",
    "                oversampled = minority_samples\n",
    "                for _ in range(multiplier - 1):  # -1 because we already have one copy\n",
    "                    oversampled = oversampled.union(minority_samples)\n",
    "                \n",
    "                # Add the remainder using sampling\n",
    "                if remainder_fraction > 0:\n",
    "                    remainder = minority_samples.sample(True, remainder_fraction)\n",
    "                    oversampled = oversampled.union(remainder)\n",
    "                \n",
    "                # Remove original instances of this class\n",
    "                result = result.filter(col(\"label\") != class_label).union(oversampled)\n",
    "                \n",
    "                print(f\"Oversampled class {class_label} from {count} to approximately {target_count} samples\")\n",
    "        \n",
    "        processed_data = result\n",
    "    \n",
    "    # Check the resulting class distribution\n",
    "    if method is not None:\n",
    "        print(\"\\nClass distribution after imbalance handling:\")\n",
    "        rebalanced_counts = processed_data.groupBy(\"label\").count().orderBy(\"label\").collect()\n",
    "        \n",
    "        for row in rebalanced_counts:\n",
    "            print(f\"Class {row['label']}: {row['count']} samples\")\n",
    "    \n",
    "    return processed_data, class_weights\n",
    "\n",
    "# Apply class imbalance handling to training data\n",
    "if IMBALANCE_CONFIG['method'] is not None:\n",
    "    print(\"\\nApplying class imbalance handling techniques...\")\n",
    "    balanced_train_data, class_weights = handle_class_imbalance(train_data, train_class_stats, IMBALANCE_CONFIG)\n",
    "    \n",
    "    # Replace original training data with balanced version\n",
    "    train_data = balanced_train_data\n",
    "else:\n",
    "    print(\"\\nNo class imbalance handling techniques applied.\")\n",
    "    class_weights = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Random Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_search(model, param_grid, train_data, val_data, num_folds=3, parallelism=1, \n",
    "                          use_f1=True, class_weights=None):\n",
    "    \"\"\"Perform random search for hyperparameter tuning of a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Machine learning model instance\n",
    "        param_grid: Parameter grid for random search\n",
    "        train_data: Training data DataFrame\n",
    "        val_data: Validation data DataFrame\n",
    "        num_folds: Number of cross-validation folds\n",
    "        parallelism: Number of parallel tasks\n",
    "        use_f1: Whether to use F1 score (True) or AUC (False) for evaluation\n",
    "        class_weights: Dictionary of class weights to use\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, best_params, train_predictions, val_predictions, train_f1, val_f1)\n",
    "    \"\"\"\n",
    "    # Initialize the evaluator based on chosen metric\n",
    "    if use_f1:\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol='label', \n",
    "            predictionCol='prediction', \n",
    "            metricName='f1'\n",
    "        )\n",
    "    else:\n",
    "        # For binary classification or when we care more about ranking\n",
    "        evaluator = BinaryClassificationEvaluator(\n",
    "            labelCol='label',\n",
    "            rawPredictionCol='rawPrediction',\n",
    "            metricName='areaUnderPR'  # Area under precision-recall curve is better for imbalanced data\n",
    "        )\n",
    "    \n",
    "    # Initialize CrossValidator for hyperparameter tuning\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        parallelism=parallelism\n",
    "    )\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the cross-validator to the training data\n",
    "    print(\"Training model with random search...\")\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # End timing\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = {}\n",
    "    for param in best_model.extractParamMap():\n",
    "        param_name = param.name\n",
    "        param_value = best_model.getOrDefault(param)\n",
    "        best_params[param_name] = param_value\n",
    "    \n",
    "    # Make predictions with the best model\n",
    "    train_predictions = best_model.transform(train_data)\n",
    "    val_predictions = best_model.transform(val_data)\n",
    "    \n",
    "    # Use F1 for final evaluation to be consistent\n",
    "    f1_evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', \n",
    "        predictionCol='prediction', \n",
    "        metricName='f1'\n",
    "    )\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    train_f1 = f1_evaluator.evaluate(train_predictions)\n",
    "    val_f1 = f1_evaluator.evaluate(val_predictions)\n",
    "    \n",
    "    return best_model, best_params, train_predictions, val_predictions, train_f1, val_f1\n",
    "\n",
    "# Special function for MLP which needs custom random search due to layers parameter\n",
    "def perform_mlp_random_search(train_data, val_data, num_features, num_classes):\n",
    "    \"\"\"Perform custom random search for MLP hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training data DataFrame\n",
    "        val_data: Validation data DataFrame\n",
    "        num_features: Number of input features\n",
    "        num_classes: Number of classes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, best_params, train_predictions, val_predictions, train_f1, val_f1)\n",
    "    \"\"\"\n",
    "    # Initialize the evaluator for F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', \n",
    "        predictionCol='prediction', \n",
    "        metricName='f1'\n",
    "    )\n",
    "    \n",
    "    # Define different network architectures for random search\n",
    "    layers_options = [\n",
    "        [num_features, num_features, num_classes],  # Simple network\n",
    "        [num_features, num_features * 2, num_features, num_classes],  # Medium network\n",
    "        [num_features, num_features * 2, num_features * 2, num_features, num_classes]  # Complex network\n",
    "    ]\n",
    "    \n",
    "    # Define parameter combinations\n",
    "    block_sizes = [64, 128, 256]\n",
    "    max_iters = [50, 100]\n",
    "    learning_rates = [0.01, 0.03, 0.1]\n",
    "    \n",
    "    # Track best model and score\n",
    "    best_mlp_model = None\n",
    "    best_mlp_val_f1 = 0\n",
    "    best_mlp_params = {}\n",
    "    best_train_predictions = None\n",
    "    best_val_predictions = None\n",
    "    best_mlp_train_f1 = 0\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Training MLP models with random search...\")\n",
    "    total_combinations = len(layers_options) * len(block_sizes) * len(max_iters) * len(learning_rates)\n",
    "    current_combination = 0\n",
    "    \n",
    "    # Manually iterate through parameter combinations\n",
    "    for layers in layers_options:\n",
    "        for block_size in block_sizes:\n",
    "            for max_iter in max_iters:\n",
    "                for step_size in learning_rates:\n",
    "                    current_combination += 1\n",
    "                    print(f\"\\rTrying combination {current_combination}/{total_combinations}\", end=\"\")\n",
    "                    \n",
    "                    # Initialize MLP with current parameters\n",
    "                    mlp = MultilayerPerceptronClassifier(\n",
    "                        labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        layers=layers,\n",
    "                        blockSize=block_size,\n",
    "                        maxIter=max_iter,\n",
    "                        stepSize=step_size,\n",
    "                        seed=42\n",
    "                    )\n",
    "                    \n",
    "                    # Train and evaluate the model\n",
    "                    mlp_model = mlp.fit(train_data)\n",
    "                    train_predictions = mlp_model.transform(train_data)\n",
    "                    val_predictions = mlp_model.transform(val_data)\n",
    "                    mlp_train_f1 = evaluator.evaluate(train_predictions)\n",
    "                    mlp_val_f1 = evaluator.evaluate(val_predictions)\n",
    "                    \n",
    "                    # Update best model if this one is better\n",
    "                    if mlp_val_f1 > best_mlp_val_f1:\n",
    "                        best_mlp_val_f1 = mlp_val_f1\n",
    "                        best_mlp_train_f1 = mlp_train_f1\n",
    "                        best_mlp_model = mlp_model\n",
    "                        best_train_predictions = train_predictions\n",
    "                        best_val_predictions = val_predictions\n",
    "                        best_mlp_params = {\n",
    "                            'layers': layers,\n",
    "                            'blockSize': block_size,\n",
    "                            'maxIter': max_iter,\n",
    "                            'stepSize': step_size\n",
    "                        }\n",
    "    \n",
    "    # End timing\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return best_mlp_model, best_mlp_params, best_train_predictions, best_val_predictions, best_mlp_train_f1, best_mlp_val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert PySpark predictions to numpy arrays for plotting\n",
    "def get_prediction_labels(predictions_df):\n",
    "    \"\"\"Extract prediction and label columns from PySpark DataFrame.\"\"\"\n",
    "    pred_labels = predictions_df.select(\"prediction\", \"label\").toPandas()\n",
    "    y_pred = pred_labels[\"prediction\"].values\n",
    "    y_true = pred_labels[\"label\"].values\n",
    "    return y_pred, y_true\n",
    "\n",
    "# Helper function to get prediction probabilities\n",
    "def get_prediction_probabilities(predictions_df):\n",
    "    \"\"\"Extract probability column from PySpark DataFrame.\"\"\"\n",
    "    # Handle the warning about Arrow conversion by manually converting to NumPy\n",
    "    probability_rows = predictions_df.select(\"probability\").collect()\n",
    "    return np.array([row.probability.toArray() for row in probability_rows])\n",
    "\n",
    "# Helper function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\", class_names=None, normalize=False):\n",
    "    \"\"\"Plot confusion matrix using seaborn with enhanced visualization.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalization option\n",
    "    if normalize:\n",
    "        cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "        cm_percentage = cm.astype('float') / cm_sum * 100\n",
    "        fmt = '.1f'\n",
    "        cm_display = cm_percentage\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "        cm_display = cm\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a more detailed heatmap\n",
    "    ax = sns.heatmap(cm_display, annot=True, fmt=fmt, cmap=\"Blues\", cbar=True,\n",
    "                     linewidths=1, linecolor='black')\n",
    "    \n",
    "    # Add row and column totals\n",
    "    row_sums = np.sum(cm, axis=1)\n",
    "    col_sums = np.sum(cm, axis=0)\n",
    "    \n",
    "    # Add class names if provided\n",
    "    if class_names is not None:\n",
    "        tick_labels = class_names\n",
    "    else:\n",
    "        tick_labels = np.arange(len(cm))\n",
    "    \n",
    "    plt.xticks(np.arange(len(tick_labels)) + 0.5, tick_labels)\n",
    "    plt.yticks(np.arange(len(tick_labels)) + 0.5, tick_labels)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Add accuracy text\n",
    "    accuracy = np.sum(np.diag(cm)) / np.sum(cm)\n",
    "    plt.text(len(cm)/2, -0.5, f\"Accuracy: {accuracy:.4f}\", ha='center', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also show the normalized version if not already showing it\n",
    "    if not normalize:\n",
    "        plot_confusion_matrix(y_true, y_pred, f\"{title} (Normalized %)\", class_names, normalize=True)\n",
    "\n",
    "# Helper function to print classification report\n",
    "def print_classification_report(y_true, y_pred, class_names=None):\n",
    "    \"\"\"Print classification report with precision, recall, and F1 scores.\"\"\"\n",
    "    target_names = class_names if class_names is not None else None\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "    support = {}\n",
    "    \n",
    "    for class_idx in np.unique(y_true):\n",
    "        true_positives = np.sum((y_true == class_idx) & (y_pred == class_idx))\n",
    "        false_positives = np.sum((y_true != class_idx) & (y_pred == class_idx))\n",
    "        false_negatives = np.sum((y_true == class_idx) & (y_pred != class_idx))\n",
    "        \n",
    "        class_support = np.sum(y_true == class_idx)\n",
    "        \n",
    "        precision[class_idx] = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall[class_idx] = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1[class_idx] = 2 * precision[class_idx] * recall[class_idx] / (precision[class_idx] + recall[class_idx]) if (precision[class_idx] + recall[class_idx]) > 0 else 0\n",
    "        support[class_idx] = class_support\n",
    "    \n",
    "    # Visual representation of per-class metrics\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique classes ensuring they're sorted\n",
    "    classes = sorted(np.unique(y_true))\n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    # Plot bars for each metric\n",
    "    precision_vals = [precision[cls] for cls in classes]\n",
    "    recall_vals = [recall[cls] for cls in classes]\n",
    "    f1_vals = [f1[cls] for cls in classes]\n",
    "    \n",
    "    plt.bar(x - width, precision_vals, width, label='Precision')\n",
    "    plt.bar(x, recall_vals, width, label='Recall')\n",
    "    plt.bar(x + width, f1_vals, width, label='F1')\n",
    "    \n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Per-Class Performance Metrics')\n",
    "    plt.xticks(x, [f'Class {cls}' for cls in classes])\n",
    "    plt.ylim(0, 1.1)  # Metrics are between 0 and 1\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display a bar chart of support (class frequencies)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    support_vals = [support[cls] for cls in classes]\n",
    "    plt.bar(x, support_vals)\n",
    "    plt.ylabel('Support (Number of Samples)')\n",
    "    plt.title('Class Distribution in Evaluation Set')\n",
    "    plt.xticks(x, [f'Class {cls}' for cls in classes])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(support_vals):\n",
    "        plt.text(i, v + 0.5, str(v), ha='center')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to plot ROC curve for multi-class\n",
    "def plot_roc_curve(y_true, y_pred_proba, n_classes):\n",
    "    \"\"\"Plot ROC curve for multi-class classification.\"\"\"\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Convert to one-hot encoding for ROC calculation\n",
    "    y_true_onehot = np.zeros((len(y_true), n_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_onehot[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Use a colormap for better differentiation between classes\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n_classes))\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
    "    \n",
    "    # Plot the diagonal\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    \n",
    "    # Set plot properties\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the average AUC\n",
    "    return np.mean(list(roc_auc.values()))\n",
    "\n",
    "# Helper function to plot precision-recall curve\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, n_classes):\n",
    "    \"\"\"Plot Precision-Recall curve for multi-class classification.\"\"\"\n",
    "    # Compute precision-recall pairs for different probability thresholds\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    avg_precision = dict()\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y_true_onehot = np.zeros((len(y_true), n_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_onehot[:, i], y_pred_proba[:, i])\n",
    "        avg_precision[i] = np.mean(precision[i])\n",
    "    \n",
    "    # Plot precision-recall curve for each class\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Use a colormap for better differentiation between classes\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, n_classes))\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                 label=f'Class {i} (avg precision = {avg_precision[i]:.2f})')\n",
    "    \n",
    "    # Set plot properties\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Multi-class Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(list(avg_precision.values()))\n",
    "\n",
    "def evaluate_model(model_name, val_predictions, num_classes, has_probability=True):\n",
    "    \"\"\"Perform comprehensive evaluation of a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        val_predictions: PySpark DataFrame with predictions\n",
    "        num_classes: Number of classes\n",
    "        has_probability: Whether the model outputs probability scores\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (y_pred, y_true, y_pred_proba)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    \n",
    "    # Calculate various metrics\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='prediction', metricName='weightedPrecision')\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='prediction', metricName='weightedRecall')\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "    \n",
    "    f1 = evaluator_f1.evaluate(val_predictions)\n",
    "    precision = evaluator_precision.evaluate(val_predictions)\n",
    "    recall = evaluator_recall.evaluate(val_predictions)\n",
    "    accuracy = evaluator_accuracy.evaluate(val_predictions)\n",
    "    \n",
    "    print(f\"\\nOverall Metrics:\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Extract predictions and true labels\n",
    "    y_pred, y_true = get_prediction_labels(val_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    plot_confusion_matrix(y_true, y_pred, f\"{model_name} Confusion Matrix\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print_classification_report(y_true, y_pred)\n",
    "    \n",
    "    # If model has probability outputs, plot ROC and PR curves\n",
    "    y_pred_proba = None\n",
    "    if has_probability:\n",
    "        try:\n",
    "            y_pred_proba = get_prediction_probabilities(val_predictions)\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            print(\"\\nROC Curve:\")\n",
    "            auc_score = plot_roc_curve(y_true, y_pred_proba, num_classes)\n",
    "            print(f\"{model_name} Average AUC: {auc_score:.4f}\")\n",
    "            \n",
    "            # Plot Precision-Recall curve\n",
    "            print(\"\\nPrecision-Recall Curve:\")\n",
    "            avg_precision = plot_precision_recall_curve(y_true, y_pred_proba, num_classes)\n",
    "            print(f\"{model_name} Average Precision: {avg_precision:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating probability-based metrics: {str(e)}\")\n",
    "            y_pred_proba = None\n",
    "    \n",
    "    return y_pred, y_true, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['logistic_regression']:\n",
    "    print(\"\\n==== Logistic Regression Model ====\")\n",
    "    \n",
    "    # Initialize the Logistic Regression model with optional class weights\n",
    "    if IMBALANCE_CONFIG.get('use_class_weights', False) and 'weight' in train_data.columns:\n",
    "        log_reg = LogisticRegression(labelCol='label', featuresCol='features', \n",
    "                                    predictionCol='prediction', weightCol='weight')\n",
    "        print(\"Using class weights for Logistic Regression\")\n",
    "    else:\n",
    "        log_reg = LogisticRegression(labelCol='label', featuresCol='features', \n",
    "                                    predictionCol='prediction')\n",
    "    \n",
    "    # Define the parameter grid for logistic regression\n",
    "    lr_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(log_reg.regParam, [0.01, 0.1, 1.0]) \\\n",
    "        .addGrid(log_reg.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .addGrid(log_reg.maxIter, [10, 20]) \\\n",
    "        .addGrid(log_reg.family, [\"multinomial\"]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    lr_best_model, lr_best_params, lr_train_preds, lr_val_preds, lr_train_f1, lr_val_f1 = perform_random_search(\n",
    "        log_reg, \n",
    "        lr_param_grid, \n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism'],\n",
    "        use_f1=True,  # Use F1 for imbalanced classification\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest Logistic Regression Parameters:\")\n",
    "    for param, value in lr_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLogistic Regression - Training F1 Score: {lr_train_f1:.4f}\")\n",
    "    print(f\"Logistic Regression - Validation F1 Score: {lr_val_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    lr_y_pred, lr_y_true, lr_y_pred_proba = evaluate_model(\"Logistic Regression\", lr_val_preds, num_classes)\n",
    "else:\n",
    "    print(\"Skipping Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['random_forest']:\n",
    "    print(\"\\n==== Random Forest Model ====\")\n",
    "    \n",
    "    # Initialize Random Forest Classifier with class weights if available\n",
    "    if IMBALANCE_CONFIG.get('use_class_weights', False) and 'weight' in train_data.columns:\n",
    "        rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\")\n",
    "        print(\"Using class weights for Random Forest\")\n",
    "    else:\n",
    "        rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define parameter grid for Random Forest\n",
    "    rf_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [50, 100]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "        .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
    "        .addGrid(rf.minInstancesPerNode, [1, 2]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    rf_best_model, rf_best_params, rf_train_preds, rf_val_preds, rf_train_f1, rf_val_f1 = perform_random_search(\n",
    "        rf, \n",
    "        rf_param_grid, \n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism'],\n",
    "        use_f1=True,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest Random Forest Parameters:\")\n",
    "    for param, value in rf_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nRandom Forest - Training F1 Score: {rf_train_f1:.4f}\")\n",
    "    print(f\"Random Forest - Validation F1 Score: {rf_val_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    rf_y_pred, rf_y_true, rf_y_pred_proba = evaluate_model(\"Random Forest\", rf_val_preds, num_classes)\n",
    "else:\n",
    "    print(\"Skipping Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Gradient Boosted Trees (GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['gbdt']:\n",
    "    print(\"\\n==== Gradient Boosted Trees Model ====\")\n",
    "    \n",
    "    # Initialize GBT Classifier with class weights if available\n",
    "    if IMBALANCE_CONFIG.get('use_class_weights', False) and 'weight' in train_data.columns:\n",
    "        gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", weightCol=\"weight\")\n",
    "        print(\"Using class weights for GBDT\")\n",
    "    else:\n",
    "        gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define parameter grid for GBT\n",
    "    gbt_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "        .addGrid(gbt.maxDepth, [3, 5]) \\\n",
    "        .addGrid(gbt.stepSize, [0.05, 0.1]) \\\n",
    "        .addGrid(gbt.minInstancesPerNode, [1, 2]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    gbt_best_model, gbt_best_params, gbt_train_preds, gbt_val_preds, gbt_train_f1, gbt_val_f1 = perform_random_search(\n",
    "        gbt, \n",
    "        gbt_param_grid, \n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism'],\n",
    "        use_f1=True,\n",
    "        class_weights=class_weights\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest GBDT Parameters:\")\n",
    "    for param, value in gbt_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nGBDT - Training F1 Score: {gbt_train_f1:.4f}\")\n",
    "    print(f\"GBDT - Validation F1 Score: {gbt_val_f1:.4f}\")\n",
    "    \n",
    "    # GBT models don't provide probability outputs in Spark\n",
    "    gbt_y_pred, gbt_y_true, _ = evaluate_model(\"GBDT\", gbt_val_preds, num_classes, has_probability=False)\n",
    "else:\n",
    "    print(\"Skipping GBDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['mlp']:\n",
    "    print(\"\\n==== Multilayer Perceptron Model ====\")\n",
    "    \n",
    "    # Get number of features\n",
    "    num_features = len(train_data.select(\"features\").first()[0])\n",
    "    \n",
    "    # Perform custom random search for MLP\n",
    "    mlp_best_model, mlp_best_params, mlp_train_preds, mlp_val_preds, mlp_train_f1, mlp_val_f1 = perform_mlp_random_search(\n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_features, \n",
    "        num_classes\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest MLP Parameters:\")\n",
    "    for param, value in mlp_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nMLP - Training F1 Score: {mlp_train_f1:.4f}\")\n",
    "    print(f\"MLP - Validation F1 Score: {mlp_val_f1:.4f}\")\n",
    "    \n",
    "    # MLP models don't provide probability outputs in Spark\n",
    "    mlp_y_pred, mlp_y_true, _ = evaluate_model(\"MLP\", mlp_val_preds, num_classes, has_probability=False)\n",
    "else:\n",
    "    print(\"Skipping MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect model names and scores for models that were run\n",
    "model_names = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "if RUN_MODELS['logistic_regression']:\n",
    "    model_names.append(\"Logistic Regression\")\n",
    "    train_scores.append(lr_train_f1)\n",
    "    val_scores.append(lr_val_f1)\n",
    "    \n",
    "if RUN_MODELS['random_forest']:\n",
    "    model_names.append(\"Random Forest\")\n",
    "    train_scores.append(rf_train_f1)\n",
    "    val_scores.append(rf_val_f1)\n",
    "    \n",
    "if RUN_MODELS['gbdt']:\n",
    "    model_names.append(\"GBDT\")\n",
    "    train_scores.append(gbt_train_f1)\n",
    "    val_scores.append(gbt_val_f1)\n",
    "    \n",
    "if RUN_MODELS['mlp']:\n",
    "    model_names.append(\"MLP\")\n",
    "    train_scores.append(mlp_train_f1)\n",
    "    val_scores.append(mlp_val_f1)\n",
    "\n",
    "# Check if we have any models to compare\n",
    "if not model_names:\n",
    "    print(\"No models were run for comparison.\")\n",
    "else:\n",
    "    # Create a comparison DataFrame\n",
    "    model_comparison = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Training F1': train_scores,\n",
    "        'Validation F1': val_scores,\n",
    "        'Difference (Train-Val)': [train - val for train, val in zip(train_scores, val_scores)]\n",
    "    })\n",
    "    \n",
    "    # Sort by validation F1 score, descending\n",
    "    model_comparison = model_comparison.sort_values('Validation F1', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(model_comparison)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Reorder based on sorted DataFrame\n",
    "    sorted_models = model_comparison['Model'].tolist()\n",
    "    sorted_train = model_comparison['Training F1'].tolist()\n",
    "    sorted_val = model_comparison['Validation F1'].tolist()\n",
    "    \n",
    "    ind = np.arange(len(sorted_models))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(ind - width/2, sorted_train, width, label='Training F1', color='skyblue')\n",
    "    plt.bar(ind + width/2, sorted_val, width, label='Validation F1', color='salmon')\n",
    "    \n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Model Comparison (Sorted by Validation F1)')\n",
    "    plt.xticks(ind, sorted_models, rotation=15)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(sorted_train):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "        \n",
    "    for i, v in enumerate(sorted_val):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_threshold(model_name, predictions_df, y_pred_proba=None, optimize_for='f1'):\n",
    "    \"\"\"Optimize prediction threshold for imbalanced classes.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        predictions_df: PySpark DataFrame with predictions\n",
    "        y_pred_proba: Optional numpy array of prediction probabilities\n",
    "        optimize_for: Metric to optimize ('f1', 'recall', 'precision', or 'balanced')\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (optimized_df, optimal_thresholds)\n",
    "    \"\"\"\n",
    "    if y_pred_proba is None or not IMBALANCE_CONFIG.get('adjust_threshold', False):\n",
    "        print(f\"\\nSkipping threshold optimization for {model_name}\")\n",
    "        return predictions_df, None\n",
    "    \n",
    "    print(f\"\\nOptimizing thresholds for {model_name}...\")\n",
    "    \n",
    "    # Extract true labels\n",
    "    y_true = predictions_df.select(\"label\").toPandas()[\"label\"].values\n",
    "    \n",
    "    # Find optimal thresholds for each class\n",
    "    optimal_thresholds = []\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y_true_onehot = np.zeros((len(y_true), num_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    # For each class, find threshold that maximizes chosen metric\n",
    "    for i in range(num_classes):\n",
    "        best_metric = 0\n",
    "        best_threshold = 0.5  # Default\n",
    "        \n",
    "        # Try different thresholds\n",
    "        for threshold in np.arange(0.1, 0.9, 0.05):\n",
    "            # Create binary predictions using this threshold\n",
    "            pred_i = (y_pred_proba[:, i] >= threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tp = np.sum((pred_i == 1) & (y_true_onehot[:, i] == 1))\n",
    "            fp = np.sum((pred_i == 1) & (y_true_onehot[:, i] == 0))\n",
    "            fn = np.sum((pred_i == 0) & (y_true_onehot[:, i] == 1))\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            # Calculate metric based on what we're optimizing for\n",
    "            if optimize_for == 'f1':\n",
    "                metric = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            elif optimize_for == 'recall':\n",
    "                metric = recall\n",
    "            elif optimize_for == 'precision':\n",
    "                metric = precision\n",
    "            elif optimize_for == 'balanced':\n",
    "                metric = (2 * recall + precision) / 3  # Emphasize recall more\n",
    "            \n",
    "            # Update if better\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        optimal_thresholds.append(best_threshold)\n",
    "        \n",
    "        # For minority classes, we may want to lower the threshold further\n",
    "        if i > 0:  # Assuming class 0 is majority\n",
    "            optimal_thresholds[i] = max(0.1, optimal_thresholds[i] * 0.8)  # Lower by 20%\n",
    "    \n",
    "    print(f\"Optimal thresholds: {optimal_thresholds}\")\n",
    "    \n",
    "    # Create a UDF to apply these thresholds\n",
    "    def create_adjusted_prediction(prob_vector, thresholds):\n",
    "        \"\"\"Adjust prediction based on optimized thresholds.\"\"\"\n",
    "        def apply_thresholds(prob_array):\n",
    "            max_score = -1\n",
    "            max_class = 0\n",
    "            for i, (prob, threshold) in enumerate(zip(prob_array, thresholds)):\n",
    "                # Calculate score as how much it exceeds its threshold\n",
    "                score = prob / threshold if threshold > 0 else 0\n",
    "                if score > max_score:\n",
    "                    max_score = score\n",
    "                    max_class = i\n",
    "            return float(max_class)\n",
    "        return udf(apply_thresholds, returnType=DoubleType())\n",
    "    \n",
    "    # Apply the optimized thresholds\n",
    "    threshold_udf = create_adjusted_prediction(\"probability\", optimal_thresholds)\n",
    "    optimized_df = predictions_df.withColumn(\"optimized_prediction\", threshold_udf(col(\"probability\")))\n",
    "    \n",
    "    # Evaluate the optimized predictions\n",
    "    print(\"\\nEvaluation after threshold optimization:\")\n",
    "    \n",
    "    # Calculate metrics with optimized predictions\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='optimized_prediction', metricName='f1')\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='optimized_prediction', metricName='weightedPrecision')\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', predictionCol='optimized_prediction', metricName='weightedRecall')\n",
    "    \n",
    "    f1 = evaluator_f1.evaluate(optimized_df)\n",
    "    precision = evaluator_precision.evaluate(optimized_df)\n",
    "    recall = evaluator_recall.evaluate(optimized_df)\n",
    "    \n",
    "    print(f\"F1 Score (Optimized): {f1:.4f}\")\n",
    "    print(f\"Precision (Optimized): {precision:.4f}\")\n",
    "    print(f\"Recall (Optimized): {recall:.4f}\")\n",
    "    \n",
    "    # Extract and display confusion matrix\n",
    "    y_pred_opt = optimized_df.select(\"optimized_prediction\", \"label\").toPandas()\n",
    "    y_pred_opt_values = y_pred_opt[\"optimized_prediction\"].values\n",
    "    y_true_values = y_pred_opt[\"label\"].values\n",
    "    \n",
    "    print(\"\\nConfusion Matrix with Optimized Thresholds:\")\n",
    "    plot_confusion_matrix(y_true_values, y_pred_opt_values, \n",
    "                         f\"{model_name} Confusion Matrix (Optimized Thresholds)\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} Classification Report (Optimized Thresholds):\")\n",
    "    print_classification_report(y_true_values, y_pred_opt_values)\n",
    "    \n",
    "    return optimized_df, optimal_thresholds\n",
    "\n",
    "# Apply threshold optimization to models with probability output\n",
    "if RUN_MODELS['logistic_regression'] and IMBALANCE_CONFIG.get('adjust_threshold', False):\n",
    "    lr_val_preds_opt, lr_thresholds = optimize_threshold(\n",
    "        \"Logistic Regression\", lr_val_preds, lr_y_pred_proba, optimize_for='balanced')\n",
    "\n",
    "if RUN_MODELS['random_forest'] and IMBALANCE_CONFIG.get('adjust_threshold', False):\n",
    "    rf_val_preds_opt, rf_thresholds = optimize_threshold(\n",
    "        \"Random Forest\", rf_val_preds, rf_y_pred_proba, optimize_for='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Set Evaluation with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_names:  # Only run if we have models\n",
    "    # Find the best model based on validation F1 scores\n",
    "    best_model_index = val_scores.index(max(val_scores))\n",
    "    best_model_name = model_names[best_model_index]\n",
    "    print(f\"Best Model: {best_model_name} with Validation F1: {max(val_scores):.4f}\")\n",
    "    \n",
    "    # Get the corresponding model object and thresholds\n",
    "    best_thresholds = None\n",
    "    if best_model_name == \"Logistic Regression\" and RUN_MODELS['logistic_regression']:\n",
    "        best_model = lr_best_model\n",
    "        has_probability = True\n",
    "        if IMBALANCE_CONFIG.get('adjust_threshold', False):\n",
    "            best_thresholds = lr_thresholds\n",
    "    elif best_model_name == \"Random Forest\" and RUN_MODELS['random_forest']:\n",
    "        best_model = rf_best_model\n",
    "        has_probability = True\n",
    "        if IMBALANCE_CONFIG.get('adjust_threshold', False):\n",
    "            best_thresholds = rf_thresholds\n",
    "    elif best_model_name == \"GBDT\" and RUN_MODELS['gbdt']:\n",
    "        best_model = gbt_best_model\n",
    "        has_probability = False\n",
    "    elif best_model_name == \"MLP\" and RUN_MODELS['mlp']:\n",
    "        best_model = mlp_best_model\n",
    "        has_probability = False\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "    \n",
    "    # Apply optimized thresholds if available\n",
    "    if best_thresholds is not None and has_probability:\n",
    "        print(f\"\\nApplying optimized thresholds to test set predictions...\")\n",
    "        \n",
    "        # Create a UDF to apply these thresholds\n",
    "        def create_adjusted_prediction(prob_vector, thresholds):\n",
    "            \"\"\"Adjust prediction based on optimized thresholds.\"\"\"\n",
    "            def apply_thresholds(prob_array):\n",
    "                max_score = -1\n",
    "                max_class = 0\n",
    "                for i, (prob, threshold) in enumerate(zip(prob_array, thresholds)):\n",
    "                    # Calculate score as how much it exceeds its threshold\n",
    "                    score = prob / threshold if threshold > 0 else 0\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        max_class = i\n",
    "                return float(max_class)\n",
    "            return udf(apply_thresholds, returnType=DoubleType())\n",
    "        \n",
    "        # Apply the optimized thresholds\n",
    "        threshold_udf = create_adjusted_prediction(\"probability\", best_thresholds)\n",
    "        test_predictions = test_predictions.withColumn(\"prediction\", threshold_udf(col(\"probability\")))\n",
    "    \n",
    "    # Initialize the evaluator for F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_f1 = evaluator.evaluate(test_predictions)\n",
    "    print(f\"Test F1 Score with {best_model_name}: {test_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation on test set\n",
    "    print(f\"\\n--- {best_model_name} Test Set Evaluation ---\")\n",
    "    test_y_pred, test_y_true, test_y_pred_proba = evaluate_model(\n",
    "        f\"{best_model_name} (Test)\", \n",
    "        test_predictions, \n",
    "        num_classes, \n",
    "        has_probability=has_probability\n",
    "    )\n",
    "else:\n",
    "    print(\"No models were run, skipping test set evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Techniques for Handling Class Imbalance\n",
    "\n",
    "In this notebook, we implemented a comprehensive approach to deal with severe class imbalance in our classification task:\n",
    "\n",
    "1. **Class Distribution Analysis**:\n",
    "   - Identified extreme imbalance with Class 0 dominating the dataset\n",
    "   - Visualized and quantified the imbalance ratio\n",
    "\n",
    "2. **Data-Level Techniques**:\n",
    "   - **Undersampling**: Reduced the majority class to create more balanced training\n",
    "   - **Oversampling**: Increased minority class representation\n",
    "   - **Combined approach**: Applied both techniques for optimal balance\n",
    "\n",
    "3. **Algorithm-Level Techniques**:\n",
    "   - **Class Weights**: Added weights to make misclassifying minority classes more costly\n",
    "   - **Threshold Optimization**: Adjusted prediction thresholds for each class\n",
    "   - **Model Selection**: Used models that handle imbalance well (tree-based models)\n",
    "\n",
    "4. **Evaluation Metrics**:\n",
    "   - Focused on metrics beyond accuracy (F1, precision, recall)\n",
    "   - Used per-class performance visualization\n",
    "   - Enhanced confusion matrix visualization\n",
    "   - Used Precision-Recall curves instead of just ROC\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. The original highly imbalanced dataset (90%+ Class 0) led to models predicting mainly the majority class\n",
    "2. Resampling techniques significantly improved minority class detection\n",
    "3. Optimizing classification thresholds further improved recall for minority classes\n",
    "4. The best model achieved a better balance of precision and recall across all classes\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Consider feature engineering specific to minority classes\n",
    "2. Explore ensemble methods that combine multiple models\n",
    "3. Gather more examples of minority classes if possible\n",
    "4. Consider anomaly detection approaches for extremely rare classes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
