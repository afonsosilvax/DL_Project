{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Models in PySpark\n",
    "\n",
    "This notebook implements multiple classification models using PySpark for big data processing:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Gradient Boosted Decision Trees (GBDT)\n",
    "4. Multilayer Perceptron (MLP)\n",
    "5. XGBoost (dependency required)\n",
    "6. LightGBM (dependency required)\n",
    "\n",
    "Each model is evaluated with:\n",
    "- Confusion Matrix\n",
    "- Classification Report\n",
    "- ROC Curve and AUC\n",
    "- Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from math import ceil\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, when, lit, count, lag, expr\n",
    "\n",
    "# ML imports for classification\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier, OneVsRest\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Scikit-learn imports for metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ML Models Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location_train = \"dbfs:/FileStore/tables/train_df-2.csv\"\n",
    "file_location_val = \"dbfs:/FileStore/tables/val_df.csv\"\n",
    "file_location_test = \"dbfs:/FileStore/tables/test_df-2.csv\"\n",
    "\n",
    "train_data = spark.read.csv(file_location_train, header=True, inferSchema=True)\n",
    "val_data = spark.read.csv(file_location_val, header=True, inferSchema=True)\n",
    "test_data = spark.read.csv(file_location_test, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select feature columns (all except 'label', 'time', and 'file')\n",
    "feature_cols = [col for col in train_data.columns if col not in ['label', 'time', 'file']]\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "train_data = assembler.transform(train_data).select(\"features\", \"label\")\n",
    "val_data = assembler.transform(val_data).select(\"features\", \"label\")\n",
    "test_data = assembler.transform(test_data).select(\"features\", \"label\")\n",
    "\n",
    "# Display the transformed train_data to verify\n",
    "train_data.show(5)\n",
    "val_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions for Evaluation Metrics and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert PySpark predictions to numpy arrays for plotting\n",
    "def get_prediction_labels(predictions_df):\n",
    "    pred_labels = predictions_df.select(\"prediction\", \"label\").toPandas()\n",
    "    y_pred = pred_labels[\"prediction\"].values\n",
    "    y_true = pred_labels[\"label\"].values\n",
    "    return y_pred, y_true\n",
    "\n",
    "# Helper function to get prediction probabilities\n",
    "def get_prediction_probabilities(predictions_df):\n",
    "    prob_df = predictions_df.select(\"probability\").toPandas()\n",
    "    return np.array([x.toArray() for x in prob_df[\"probability\"]])\n",
    "\n",
    "# Helper function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to print classification report\n",
    "def print_classification_report(y_true, y_pred):\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Helper function to plot ROC curve for multi-class\n",
    "def plot_roc_curve(y_true, y_pred_proba, n_classes):\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Convert to one-hot encoding for ROC calculation\n",
    "    y_true_onehot = np.zeros((len(y_true), n_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_onehot[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors[:n_classes]):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'.\n",
    "                 format(i, roc_auc[i]))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the average AUC\n",
    "    return np.mean(list(roc_auc.values()))\n",
    "\n",
    "# Helper function to plot precision-recall curve\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, n_classes):\n",
    "    # Compute precision-recall pairs for different probability thresholds\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    avg_precision = dict()\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y_true_onehot = np.zeros((len(y_true), n_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_onehot[:, i], y_pred_proba[:, i])\n",
    "        avg_precision[i] = np.mean(precision[i])\n",
    "    \n",
    "    # Plot precision-recall curve for each class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors[:n_classes]):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                 label='Class {0} (avg precision = {1:0.2f})'.\n",
    "                 format(i, avg_precision[i]))\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Multi-class Precision-Recall Curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(list(avg_precision.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Model with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Random Search for Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "log_reg = LogisticRegression(labelCol='label', featuresCol='features', maxIter=5, predictionCol='prediction')\n",
    "\n",
    "# Define the parameter grid for logistic regression\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(log_reg.regParam, [0.1, 1, 10]) \\\n",
    "    .addGrid(log_reg.elasticNetParam, [0.0, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Initialize CrossValidator for hyperparameter tuning\n",
    "crossval = CrossValidator(\n",
    "    estimator=log_reg,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5,  # 5-fold cross-validation\n",
    "    parallelism=2  # Parallelism to optimize resource usage\n",
    ")\n",
    "\n",
    "# Fit the cross-validator to the training data\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Extract the best model\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Print tuned parameters\n",
    "tuned_params = ['regParam', 'elasticNetParam']\n",
    "best_params = {}\n",
    "\n",
    "for param in tuned_params:\n",
    "    if best_model.hasParam(param) and best_model.isSet(getattr(log_reg, param)):\n",
    "        best_params[param] = best_model.getOrDefault(getattr(log_reg, param))\n",
    "\n",
    "print(\"Best Tuned Parameters:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Coefficient Matrix:\")\n",
    "print(best_model.coefficientMatrix)\n",
    "print(\"Best Intercept Vector:\")\n",
    "print(best_model.interceptVector)\n",
    "\n",
    "# Print the best F1 score\n",
    "print(\"Best F1 Score:\", evaluator.evaluate(best_model.transform(val_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Logistic Regression Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "log_reg = LogisticRegression(labelCol='label', featuresCol='features', maxIter=5, family='multinomial', regParam=5, elasticNetParam=0)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_model = log_reg.fit(train_data)\n",
    "\n",
    "# Make predictions on the training and validation sets\n",
    "train_predictions = lr_model.transform(train_data)\n",
    "val_predictions = lr_model.transform(val_data)\n",
    "\n",
    "# Initialize the evaluator for F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "\n",
    "# Calculate F1 scores\n",
    "score_train = evaluator.evaluate(train_predictions)\n",
    "score_val = evaluator.evaluate(val_predictions)\n",
    "\n",
    "print(\"Training F1 Score:\", score_train)\n",
    "print(\"Validation F1 Score:\", score_val)\n",
    "\n",
    "# Extract predictions for later use\n",
    "lr_y_pred, lr_y_true = get_prediction_labels(val_predictions)\n",
    "lr_y_pred_proba = get_prediction_probabilities(val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. One vs Rest Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the logistic regression model\n",
    "log_reg = LogisticRegression(labelCol='label', featuresCol='features', maxIter=5, regParam=5, elasticNetParam=0.5)\n",
    "\n",
    "# One-vs-Rest strategy\n",
    "ovr = OneVsRest(classifier=log_reg)\n",
    "\n",
    "# Fit the model to the training set\n",
    "lr_ovr_model = ovr.fit(train_data)\n",
    "\n",
    "# Make predictions on the training and validation sets\n",
    "train_predictions_ovr = lr_ovr_model.transform(train_data)\n",
    "val_predictions_ovr = lr_ovr_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "score_train_ovr = evaluator.evaluate(train_predictions_ovr)\n",
    "score_val_ovr = evaluator.evaluate(val_predictions_ovr)\n",
    "\n",
    "print(\"OVR Training F1 Score:\", score_train_ovr)\n",
    "print(\"OVR Validation F1 Score:\", score_val_ovr)\n",
    "\n",
    "# Extract predictions for later use\n",
    "lr_ovr_y_pred, lr_ovr_y_true = get_prediction_labels(val_predictions_ovr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest Classifier\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\",\n",
    "    numTrees=100,           # Number of trees\n",
    "    maxDepth=10,            # Maximum depth of each tree \n",
    "    seed=42                 # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "rf_train_predictions = rf_model.transform(train_data)\n",
    "rf_val_predictions = rf_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "rf_train_f1 = evaluator.evaluate(rf_train_predictions)\n",
    "rf_val_f1 = evaluator.evaluate(rf_val_predictions)\n",
    "\n",
    "print(\"Random Forest - Training F1 Score:\", rf_train_f1)\n",
    "print(\"Random Forest - Validation F1 Score:\", rf_val_f1)\n",
    "\n",
    "# Extract predictions and probabilities for evaluation\n",
    "rf_y_pred, rf_y_true = get_prediction_labels(rf_val_predictions)\n",
    "rf_y_pred_proba = get_prediction_probabilities(rf_val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosted Decision Trees (GBDT) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GBT Classifier\n",
    "gbt = GBTClassifier(\n",
    "    labelCol=\"label\", \n",
    "    featuresCol=\"features\",\n",
    "    maxIter=10,             # Number of iterations\n",
    "    maxDepth=5,             # Maximum depth of each tree\n",
    "    stepSize=0.1,           # Learning rate\n",
    "    seed=42                 # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "gbt_model = gbt.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "gbt_train_predictions = gbt_model.transform(train_data)\n",
    "gbt_val_predictions = gbt_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "gbt_train_f1 = evaluator.evaluate(gbt_train_predictions)\n",
    "gbt_val_f1 = evaluator.evaluate(gbt_val_predictions)\n",
    "\n",
    "print(\"GBDT - Training F1 Score:\", gbt_train_f1)\n",
    "print(\"GBDT - Validation F1 Score:\", gbt_val_f1)\n",
    "\n",
    "# Extract predictions for later evaluation\n",
    "gbt_y_pred, gbt_y_true = get_prediction_labels(gbt_val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multilayer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of features and classes\n",
    "num_features = len(train_data.select(\"features\").first()[0])\n",
    "num_classes = train_data.select(\"label\").distinct().count()\n",
    "\n",
    "# Define the network architecture\n",
    "# Input layer: number of features\n",
    "# Hidden layers: twice the number of features, then half that\n",
    "# Output layer: number of classes\n",
    "layers = [num_features, num_features * 2, num_features, num_classes]\n",
    "\n",
    "# Initialize MLP Classifier\n",
    "mlp = MultilayerPerceptronClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    layers=layers,\n",
    "    blockSize=128,          # Block size for stochastic gradient descent\n",
    "    seed=42,                # For reproducibility\n",
    "    maxIter=100             # Maximum iterations\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "mlp_model = mlp.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "mlp_train_predictions = mlp_model.transform(train_data)\n",
    "mlp_val_predictions = mlp_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "mlp_train_f1 = evaluator.evaluate(mlp_train_predictions)\n",
    "mlp_val_f1 = evaluator.evaluate(mlp_val_predictions)\n",
    "\n",
    "print(\"MLP - Training F1 Score:\", mlp_train_f1)\n",
    "print(\"MLP - Validation F1 Score:\", mlp_val_f1)\n",
    "\n",
    "# Extract predictions for later evaluation\n",
    "mlp_y_pred, mlp_y_true = get_prediction_labels(mlp_val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. XGBoost and LightGBM (Commented Out - Requires Additional Dependencies)\n",
    "\n",
    "To use XGBoost and LightGBM with PySpark, you'll need to add the following dependencies to your cluster:\n",
    "- For XGBoost: `com.github.ozancicek:spark-xgboost_2.12:0.2.1`\n",
    "- For LightGBM: `com.microsoft.ml.spark:mmlspark_2.12:1.0.0-rc1`\n",
    "\n",
    "Uncomment the code below after adding these dependencies to your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost implementation (uncomment after adding required dependencies)\n",
    "\n",
    "'''\n",
    "from com.github.ozancicek.spark.ml.xgboost import XGBoostClassifier\n",
    "\n",
    "# Initialize XGBoost Classifier\n",
    "xgb = XGBoostClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numRound=100,\n",
    "    maxDepth=5,\n",
    "    eta=0.1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "xgb_model = xgb.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "xgb_train_predictions = xgb_model.transform(train_data)\n",
    "xgb_val_predictions = xgb_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "xgb_train_f1 = evaluator.evaluate(xgb_train_predictions)\n",
    "xgb_val_f1 = evaluator.evaluate(xgb_val_predictions)\n",
    "\n",
    "print(\"XGBoost - Training F1 Score:\", xgb_train_f1)\n",
    "print(\"XGBoost - Validation F1 Score:\", xgb_val_f1)\n",
    "\n",
    "# Extract predictions for later evaluation\n",
    "xgb_y_pred, xgb_y_true = get_prediction_labels(xgb_val_predictions)\n",
    "xgb_y_pred_proba = get_prediction_probabilities(xgb_val_predictions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM implementation (uncomment after adding required dependencies)\n",
    "\n",
    "'''\n",
    "from com.microsoft.ml.spark.lightgbm import LightGBMClassifier\n",
    "\n",
    "# Initialize LightGBM Classifier\n",
    "lgbm = LightGBMClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numLeaves=31,\n",
    "    numIterations=100,\n",
    "    learningRate=0.1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "lgbm_model = lgbm.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "lgbm_train_predictions = lgbm_model.transform(train_data)\n",
    "lgbm_val_predictions = lgbm_model.transform(val_data)\n",
    "\n",
    "# Calculate F1 scores\n",
    "lgbm_train_f1 = evaluator.evaluate(lgbm_train_predictions)\n",
    "lgbm_val_f1 = evaluator.evaluate(lgbm_val_predictions)\n",
    "\n",
    "print(\"LightGBM - Training F1 Score:\", lgbm_train_f1)\n",
    "print(\"LightGBM - Validation F1 Score:\", lgbm_val_f1)\n",
    "\n",
    "# Extract predictions for later evaluation\n",
    "lgbm_y_pred, lgbm_y_true = get_prediction_labels(lgbm_val_predictions)\n",
    "lgbm_y_pred_proba = get_prediction_probabilities(lgbm_val_predictions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluation Metrics and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each model\n",
    "print(\"Confusion Matrices:\")\n",
    "plot_confusion_matrix(lr_y_true, lr_y_pred, \"Logistic Regression Confusion Matrix\")\n",
    "plot_confusion_matrix(rf_y_true, rf_y_pred, \"Random Forest Confusion Matrix\")\n",
    "plot_confusion_matrix(gbt_y_true, gbt_y_pred, \"GBDT Confusion Matrix\")\n",
    "plot_confusion_matrix(mlp_y_true, mlp_y_pred, \"MLP Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Classification Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification reports for each model\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print_classification_report(lr_y_true, lr_y_pred)\n",
    "\n",
    "print(\"\\nRandom Forest Classification Report:\")\n",
    "print_classification_report(rf_y_true, rf_y_pred)\n",
    "\n",
    "print(\"\\nGBDT Classification Report:\")\n",
    "print_classification_report(gbt_y_true, gbt_y_pred)\n",
    "\n",
    "print(\"\\nMLP Classification Report:\")\n",
    "print_classification_report(mlp_y_true, mlp_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. ROC Curves and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of classes for ROC curves\n",
    "num_classes = int(train_data.select(\"label\").distinct().count())\n",
    "\n",
    "# Plot ROC curves\n",
    "print(\"ROC Curves and AUC:\")\n",
    "lr_auc = plot_roc_curve(lr_y_true, lr_y_pred_proba, num_classes)\n",
    "print(f\"Logistic Regression Average AUC: {lr_auc:.4f}\")\n",
    "\n",
    "rf_auc = plot_roc_curve(rf_y_true, rf_y_pred_proba, num_classes)\n",
    "print(f\"Random Forest Average AUC: {rf_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "print(\"Precision-Recall Curves:\")\n",
    "lr_avg_prec = plot_precision_recall_curve(lr_y_true, lr_y_pred_proba, num_classes)\n",
    "print(f\"Logistic Regression Average Precision: {lr_avg_prec:.4f}\")\n",
    "\n",
    "rf_avg_prec = plot_precision_recall_curve(rf_y_true, rf_y_pred_proba, num_classes)\n",
    "print(f\"Random Forest Average Precision: {rf_avg_prec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "models = [\"Logistic Regression\", \"Logistic Regression (OVR)\", \"Random Forest\", \"GBDT\", \"MLP\"]\n",
    "train_scores = [score_train, score_train_ovr, rf_train_f1, gbt_train_f1, mlp_train_f1]\n",
    "val_scores = [score_val, score_val_ovr, rf_val_f1, gbt_val_f1, mlp_val_f1]\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'Training F1': train_scores,\n",
    "    'Validation F1': val_scores\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Plot model comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "ind = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(ind - width/2, train_scores, width, label='Training F1')\n",
    "plt.bar(ind + width/2, val_scores, width, label='Validation F1')\n",
    "\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Model Comparison')\n",
    "plt.xticks(ind, models, rotation=15)\n",
    "plt.legend(loc='best')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(train_scores):\n",
    "    plt.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "for i, v in enumerate(val_scores):\n",
    "    plt.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Set Evaluation with Best Model\n",
    "\n",
    "Based on the validation results, let's evaluate the best performing model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model based on validation F1 scores\n",
    "best_model_index = val_scores.index(max(val_scores))\n",
    "best_model_name = models[best_model_index]\n",
    "print(f\"Best Model: {best_model_name} with Validation F1: {max(val_scores):.4f}\")\n",
    "\n",
    "# Get the corresponding model object\n",
    "if best_model_name == \"Logistic Regression\":\n",
    "    best_model = lr_model\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "elif best_model_name == \"Logistic Regression (OVR)\":\n",
    "    best_model = lr_ovr_model\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "elif best_model_name == \"Random Forest\":\n",
    "    best_model = rf_model\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "elif best_model_name == \"GBDT\":\n",
    "    best_model = gbt_model\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "elif best_model_name == \"MLP\":\n",
    "    best_model = mlp_model\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_f1 = evaluator.evaluate(test_predictions)\n",
    "print(f\"Test F1 Score with {best_model_name}: {test_f1:.4f}\")\n",
    "\n",
    "# Get predictions and true labels\n",
    "test_y_pred, test_y_true = get_prediction_labels(test_predictions)\n",
    "\n",
    "# Plot confusion matrix for test set\n",
    "plot_confusion_matrix(test_y_true, test_y_pred, f\"{best_model_name} Confusion Matrix (Test Set)\")\n",
    "\n",
    "# Print classification report for test set\n",
    "print(f\"\\n{best_model_name} Classification Report (Test Set):\")\n",
    "print_classification_report(test_y_true, test_y_pred)\n",
    "\n",
    "# If model provides probability predictions, plot ROC and PR curves\n",
    "if best_model_name in [\"Logistic Regression\", \"Random Forest\"]:\n",
    "    test_y_pred_proba = get_prediction_probabilities(test_predictions)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    test_auc = plot_roc_curve(test_y_true, test_y_pred_proba, num_classes)\n",
    "    print(f\"{best_model_name} Average AUC (Test Set): {test_auc:.4f}\")\n",
    "    \n",
    "    # Plot Precision-Recall curve\n",
    "    test_avg_prec = plot_precision_recall_curve(test_y_true, test_y_pred_proba, num_classes)\n",
    "    print(f\"{best_model_name} Average Precision (Test Set): {test_avg_prec:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've implemented and evaluated multiple classification models using PySpark:\n",
    "\n",
    "1. Logistic Regression (standard and One-vs-Rest)\n",
    "2. Random Forest\n",
    "3. Gradient Boosted Decision Trees (GBDT)\n",
    "4. Multilayer Perceptron (MLP)\n",
    "\n",
    "We've also provided code for XGBoost and LightGBM which require additional dependencies.\n",
    "\n",
    "For each model, we've evaluated:\n",
    "- Training and validation F1 scores\n",
    "- Confusion matrices\n",
    "- Detailed classification reports\n",
    "- ROC curves and AUC values\n",
    "- Precision-Recall curves\n",
    "\n",
    "The best performing model was evaluated on the test set to provide a final assessment of its performance.\n",
    "\n",
    "This comprehensive evaluation allows for informed model selection based on the specific requirements of the classification task."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
