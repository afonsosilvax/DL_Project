{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning Models in PySpark\n",
    "\n",
    "This notebook implements multiple classification models using PySpark with random search hyperparameter optimization.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Model Selection Configuration](#1.-Model-Selection-Configuration)\n",
    "2. [Importing Libraries](#2.-Importing-Libraries)\n",
    "3. [Loading and Preprocessing Data](#3.-Loading-and-Preprocessing-Data)\n",
    "4. [Helper Functions](#4.-Helper-Functions)\n",
    "    - [4.1 Random Search Function](#4.1-Random-Search-Function)\n",
    "    - [4.2 Evaluation Functions](#4.2-Evaluation-Functions)\n",
    "5. [Model Training](#5.-Model-Training)\n",
    "    - [5.1 Logistic Regression](#5.1-Logistic-Regression)\n",
    "    - [5.2 Random Forest](#5.2-Random-Forest)\n",
    "    - [5.3 Gradient Boosted Trees](#5.3-Gradient-Boosted-Trees-(GBDT))\n",
    "    - [5.4 Multilayer Perceptron](#5.4-Multilayer-Perceptron-(MLP))\n",
    "6. [Model Comparison](#6.-Model-Comparison)\n",
    "7. [Test Set Evaluation](#7.-Test-Set-Evaluation-with-Best-Model)\n",
    "8. [Conclusion](#8.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Selection Configuration\n",
    "\n",
    "**Set to True for models you want to train, False for models to skip.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection configuration - Set to True to run, False to skip\n",
    "RUN_MODELS = {\n",
    "    'logistic_regression': True,\n",
    "    'random_forest': True,\n",
    "    'gbdt': True,\n",
    "    'mlp': True\n",
    "}\n",
    "\n",
    "# Random search configuration\n",
    "RANDOM_SEARCH_CONFIG = {\n",
    "    'num_folds': 3,      # Number of folds for cross-validation\n",
    "    'parallelism': 2     # Number of parallel tasks during tuning\n",
    "}\n",
    "\n",
    "# File paths configuration\n",
    "DATA_PATHS = {\n",
    "    'train': \"dbfs:/FileStore/tables/train_df-2.csv\",\n",
    "    'val': \"dbfs:/FileStore/tables/val_df.csv\",\n",
    "    'test': \"dbfs:/FileStore/tables/test_df-2.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from math import ceil\n",
    "import time\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import SparkSession  \n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number, when, lit, count, lag, expr\n",
    "\n",
    "# ML imports for classification\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# Scikit-learn imports for metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ML Models Spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading and Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_paths):\n",
    "    \"\"\"Load and preprocess data from specified file paths.\n",
    "    \n",
    "    Args:\n",
    "        file_paths (dict): Dictionary with keys 'train', 'val', 'test' and file path values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Preprocessed train, validation, and test data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    train_data = spark.read.csv(file_paths['train'], header=True, inferSchema=True)\n",
    "    val_data = spark.read.csv(file_paths['val'], header=True, inferSchema=True)\n",
    "    test_data = spark.read.csv(file_paths['test'], header=True, inferSchema=True)\n",
    "    \n",
    "    # Select feature columns (all except 'label', 'time', and 'file')\n",
    "    feature_cols = [col for col in train_data.columns if col not in ['label', 'time', 'file']]\n",
    "    \n",
    "    # Assemble features into a single vector column\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    train_data = assembler.transform(train_data).select(\"features\", \"label\")\n",
    "    val_data = assembler.transform(val_data).select(\"features\", \"label\")\n",
    "    test_data = assembler.transform(test_data).select(\"features\", \"label\")\n",
    "    \n",
    "    print(f\"Data loaded and preprocessed:\")\n",
    "    print(f\"  - Training samples: {train_data.count()}\")\n",
    "    print(f\"  - Validation samples: {val_data.count()}\")\n",
    "    print(f\"  - Test samples: {test_data.count()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_data, val_data, test_data = load_and_preprocess_data(DATA_PATHS)\n",
    "\n",
    "# Display a few samples\n",
    "print(\"\\nSample of training data:\")\n",
    "train_data.show(3)\n",
    "\n",
    "# Count classes\n",
    "num_classes = train_data.select(\"label\").distinct().count()\n",
    "print(f\"\\nNumber of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_random_search(model, param_grid, train_data, val_data, num_folds=3, parallelism=2):\n",
    "    \"\"\"Perform random search for hyperparameter tuning of a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Machine learning model instance\n",
    "        param_grid: Parameter grid for random search\n",
    "        train_data: Training data DataFrame\n",
    "        val_data: Validation data DataFrame\n",
    "        num_folds: Number of cross-validation folds\n",
    "        parallelism: Number of parallel tasks\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, best_params, train_predictions, val_predictions, train_f1, val_f1)\n",
    "    \"\"\"\n",
    "    # Initialize the evaluator for F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', \n",
    "        predictionCol='prediction', \n",
    "        metricName='f1'\n",
    "    )\n",
    "    \n",
    "    # Initialize CrossValidator for hyperparameter tuning\n",
    "    cv = CrossValidator(\n",
    "        estimator=model,\n",
    "        estimatorParamMaps=param_grid,\n",
    "        evaluator=evaluator,\n",
    "        numFolds=num_folds,\n",
    "        parallelism=parallelism\n",
    "    )\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the cross-validator to the training data\n",
    "    print(\"Training model with random search...\")\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # End timing\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Extract the best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Get best parameters\n",
    "    best_params = {}\n",
    "    for param in best_model.extractParamMap():\n",
    "        param_name = param.name\n",
    "        param_value = best_model.getOrDefault(param)\n",
    "        best_params[param_name] = param_value\n",
    "    \n",
    "    # Make predictions with the best model\n",
    "    train_predictions = best_model.transform(train_data)\n",
    "    val_predictions = best_model.transform(val_data)\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    train_f1 = evaluator.evaluate(train_predictions)\n",
    "    val_f1 = evaluator.evaluate(val_predictions)\n",
    "    \n",
    "    return best_model, best_params, train_predictions, val_predictions, train_f1, val_f1\n",
    "\n",
    "# Special function for MLP which needs custom random search due to layers parameter\n",
    "def perform_mlp_random_search(train_data, val_data, num_features, num_classes):\n",
    "    \"\"\"Perform custom random search for MLP hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        train_data: Training data DataFrame\n",
    "        val_data: Validation data DataFrame\n",
    "        num_features: Number of input features\n",
    "        num_classes: Number of classes\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (best_model, best_params, train_predictions, val_predictions, train_f1, val_f1)\n",
    "    \"\"\"\n",
    "    # Initialize the evaluator for F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        labelCol='label', \n",
    "        predictionCol='prediction', \n",
    "        metricName='f1'\n",
    "    )\n",
    "    \n",
    "    # Define different network architectures for random search\n",
    "    layers_options = [\n",
    "        [num_features, num_features, num_classes],  # Simple network\n",
    "        [num_features, num_features * 2, num_features, num_classes],  # Medium network\n",
    "        [num_features, num_features * 2, num_features * 2, num_features, num_classes]  # Complex network\n",
    "    ]\n",
    "    \n",
    "    # Define parameter combinations\n",
    "    block_sizes = [64, 128, 256]\n",
    "    max_iters = [50, 100, 200]\n",
    "    learning_rates = [0.01, 0.03, 0.1]\n",
    "    \n",
    "    # Track best model and score\n",
    "    best_mlp_model = None\n",
    "    best_mlp_val_f1 = 0\n",
    "    best_mlp_params = {}\n",
    "    best_train_predictions = None\n",
    "    best_val_predictions = None\n",
    "    best_mlp_train_f1 = 0\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Training MLP models with random search...\")\n",
    "    total_combinations = len(layers_options) * len(block_sizes) * len(max_iters) * len(learning_rates)\n",
    "    current_combination = 0\n",
    "    \n",
    "    # Manually iterate through parameter combinations\n",
    "    for layers in layers_options:\n",
    "        for block_size in block_sizes:\n",
    "            for max_iter in max_iters:\n",
    "                for step_size in learning_rates:\n",
    "                    current_combination += 1\n",
    "                    print(f\"\\rTrying combination {current_combination}/{total_combinations}\", end=\"\")\n",
    "                    \n",
    "                    # Initialize MLP with current parameters\n",
    "                    mlp = MultilayerPerceptronClassifier(\n",
    "                        labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        layers=layers,\n",
    "                        blockSize=block_size,\n",
    "                        maxIter=max_iter,\n",
    "                        stepSize=step_size,\n",
    "                        seed=42\n",
    "                    )\n",
    "                    \n",
    "                    # Train and evaluate the model\n",
    "                    mlp_model = mlp.fit(train_data)\n",
    "                    train_predictions = mlp_model.transform(train_data)\n",
    "                    val_predictions = mlp_model.transform(val_data)\n",
    "                    mlp_train_f1 = evaluator.evaluate(train_predictions)\n",
    "                    mlp_val_f1 = evaluator.evaluate(val_predictions)\n",
    "                    \n",
    "                    # Update best model if this one is better\n",
    "                    if mlp_val_f1 > best_mlp_val_f1:\n",
    "                        best_mlp_val_f1 = mlp_val_f1\n",
    "                        best_mlp_train_f1 = mlp_train_f1\n",
    "                        best_mlp_model = mlp_model\n",
    "                        best_train_predictions = train_predictions\n",
    "                        best_val_predictions = val_predictions\n",
    "                        best_mlp_params = {\n",
    "                            'layers': layers,\n",
    "                            'blockSize': block_size,\n",
    "                            'maxIter': max_iter,\n",
    "                            'stepSize': step_size\n",
    "                        }\n",
    "    \n",
    "    # End timing\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return best_mlp_model, best_mlp_params, best_train_predictions, best_val_predictions, best_mlp_train_f1, best_mlp_val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert PySpark predictions to numpy arrays for plotting\n",
    "def get_prediction_labels(predictions_df):\n",
    "    \"\"\"Extract prediction and label columns from PySpark DataFrame.\"\"\"\n",
    "    pred_labels = predictions_df.select(\"prediction\", \"label\").toPandas()\n",
    "    y_pred = pred_labels[\"prediction\"].values\n",
    "    y_true = pred_labels[\"label\"].values\n",
    "    return y_pred, y_true\n",
    "\n",
    "# Helper function to get prediction probabilities\n",
    "def get_prediction_probabilities(predictions_df):\n",
    "    \"\"\"Extract probability column from PySpark DataFrame.\"\"\"\n",
    "    prob_df = predictions_df.select(\"probability\").toPandas()\n",
    "    return np.array([x.toArray() for x in prob_df[\"probability\"]])\n",
    "\n",
    "# Helper function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Plot confusion matrix using seaborn.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to print classification report\n",
    "def print_classification_report(y_true, y_pred):\n",
    "    \"\"\"Print classification report with precision, recall, and F1 scores.\"\"\"\n",
    "    report = classification_report(y_true, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "# Helper function to plot ROC curve for multi-class\n",
    "def plot_roc_curve(y_true, y_pred_proba, n_classes):\n",
    "    \"\"\"Plot ROC curve for multi-class classification.\"\"\"\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    # Convert to one-hot encoding for ROC calculation\n",
    "    y_true_onehot = np.zeros((len(y_true), n_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_onehot[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Plot all ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors[:n_classes]):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                 label='ROC curve of class {0} (area = {1:0.2f})'.\n",
    "                 format(i, roc_auc[i]))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Multi-class ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Return the average AUC\n",
    "    return np.mean(list(roc_auc.values()))\n",
    "\n",
    "# Helper function to plot precision-recall curve\n",
    "def plot_precision_recall_curve(y_true, y_pred_proba, n_classes):\n",
    "    \"\"\"Plot Precision-Recall curve for multi-class classification.\"\"\"\n",
    "    # Compute precision-recall pairs for different probability thresholds\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    avg_precision = dict()\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y_true_onehot = np.zeros((len(y_true), n_classes))\n",
    "    for i in range(len(y_true)):\n",
    "        y_true_onehot[i, int(y_true[i])] = 1\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_onehot[:, i], y_pred_proba[:, i])\n",
    "        avg_precision[i] = np.mean(precision[i])\n",
    "    \n",
    "    # Plot precision-recall curve for each class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, color in zip(range(n_classes), colors[:n_classes]):\n",
    "        plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                 label='Class {0} (avg precision = {1:0.2f})'.\n",
    "                 format(i, avg_precision[i]))\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Multi-class Precision-Recall Curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(list(avg_precision.values()))\n",
    "\n",
    "def evaluate_model(model_name, val_predictions, num_classes, has_probability=True):\n",
    "    \"\"\"Perform comprehensive evaluation of a model.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        val_predictions: PySpark DataFrame with predictions\n",
    "        num_classes: Number of classes\n",
    "        has_probability: Whether the model outputs probability scores\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (y_pred, y_true, y_pred_proba)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "    \n",
    "    # Extract predictions and true labels\n",
    "    y_pred, y_true = get_prediction_labels(val_predictions)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    plot_confusion_matrix(y_true, y_pred, f\"{model_name} Confusion Matrix\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print_classification_report(y_true, y_pred)\n",
    "    \n",
    "    # If model has probability outputs, plot ROC and PR curves\n",
    "    y_pred_proba = None\n",
    "    if has_probability:\n",
    "        y_pred_proba = get_prediction_probabilities(val_predictions)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        print(\"\\nROC Curve:\")\n",
    "        auc_score = plot_roc_curve(y_true, y_pred_proba, num_classes)\n",
    "        print(f\"{model_name} Average AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        # Plot Precision-Recall curve\n",
    "        print(\"\\nPrecision-Recall Curve:\")\n",
    "        avg_precision = plot_precision_recall_curve(y_true, y_pred_proba, num_classes)\n",
    "        print(f\"{model_name} Average Precision: {avg_precision:.4f}\")\n",
    "    \n",
    "    return y_pred, y_true, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['logistic_regression']:\n",
    "    print(\"\\n==== Logistic Regression Model ====\")\n",
    "    \n",
    "    # Initialize the Logistic Regression model\n",
    "    log_reg = LogisticRegression(labelCol='label', featuresCol='features', predictionCol='prediction')\n",
    "    \n",
    "    # Define the parameter grid for logistic regression\n",
    "    lr_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(log_reg.regParam, [0.01, 0.1, 1.0, 10.0]) \\\n",
    "        .addGrid(log_reg.elasticNetParam, [0.0, 0.3, 0.7, 1.0]) \\\n",
    "        .addGrid(log_reg.maxIter, [5, 10, 20]) \\\n",
    "        .addGrid(log_reg.family, [\"multinomial\", \"auto\"]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    lr_best_model, lr_best_params, lr_train_preds, lr_val_preds, lr_train_f1, lr_val_f1 = perform_random_search(\n",
    "        log_reg, \n",
    "        lr_param_grid, \n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest Logistic Regression Parameters:\")\n",
    "    for param, value in lr_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nLogistic Regression - Training F1 Score: {lr_train_f1:.4f}\")\n",
    "    print(f\"Logistic Regression - Validation F1 Score: {lr_val_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    lr_y_pred, lr_y_true, lr_y_pred_proba = evaluate_model(\"Logistic Regression\", lr_val_preds, num_classes)\n",
    "else:\n",
    "    print(\"Skipping Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['random_forest']:\n",
    "    print(\"\\n==== Random Forest Model ====\")\n",
    "    \n",
    "    # Initialize Random Forest Classifier\n",
    "    rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define parameter grid for Random Forest\n",
    "    rf_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(rf.numTrees, [50, 100, 200]) \\\n",
    "        .addGrid(rf.maxDepth, [5, 10, 15]) \\\n",
    "        .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
    "        .addGrid(rf.minInstancesPerNode, [1, 2, 4]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    rf_best_model, rf_best_params, rf_train_preds, rf_val_preds, rf_train_f1, rf_val_f1 = perform_random_search(\n",
    "        rf, \n",
    "        rf_param_grid, \n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest Random Forest Parameters:\")\n",
    "    for param, value in rf_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nRandom Forest - Training F1 Score: {rf_train_f1:.4f}\")\n",
    "    print(f\"Random Forest - Validation F1 Score: {rf_val_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation\n",
    "    rf_y_pred, rf_y_true, rf_y_pred_proba = evaluate_model(\"Random Forest\", rf_val_preds, num_classes)\n",
    "else:\n",
    "    print(\"Skipping Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Gradient Boosted Trees (GBDT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['gbdt']:\n",
    "    print(\"\\n==== Gradient Boosted Trees Model ====\")\n",
    "    \n",
    "    # Initialize GBT Classifier\n",
    "    gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "    \n",
    "    # Define parameter grid for GBT\n",
    "    gbt_param_grid = ParamGridBuilder() \\\n",
    "        .addGrid(gbt.maxIter, [10, 20, 50]) \\\n",
    "        .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n",
    "        .addGrid(gbt.stepSize, [0.05, 0.1, 0.2]) \\\n",
    "        .addGrid(gbt.minInstancesPerNode, [1, 2, 4]) \\\n",
    "        .build()\n",
    "    \n",
    "    # Perform random search\n",
    "    gbt_best_model, gbt_best_params, gbt_train_preds, gbt_val_preds, gbt_train_f1, gbt_val_f1 = perform_random_search(\n",
    "        gbt, \n",
    "        gbt_param_grid, \n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_folds=RANDOM_SEARCH_CONFIG['num_folds'],\n",
    "        parallelism=RANDOM_SEARCH_CONFIG['parallelism']\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest GBDT Parameters:\")\n",
    "    for param, value in gbt_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nGBDT - Training F1 Score: {gbt_train_f1:.4f}\")\n",
    "    print(f\"GBDT - Validation F1 Score: {gbt_val_f1:.4f}\")\n",
    "    \n",
    "    # GBT models don't provide probability outputs in Spark\n",
    "    gbt_y_pred, gbt_y_true, _ = evaluate_model(\"GBDT\", gbt_val_preds, num_classes, has_probability=False)\n",
    "else:\n",
    "    print(\"Skipping GBDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_MODELS['mlp']:\n",
    "    print(\"\\n==== Multilayer Perceptron Model ====\")\n",
    "    \n",
    "    # Get number of features\n",
    "    num_features = len(train_data.select(\"features\").first()[0])\n",
    "    \n",
    "    # Perform custom random search for MLP\n",
    "    mlp_best_model, mlp_best_params, mlp_train_preds, mlp_val_preds, mlp_train_f1, mlp_val_f1 = perform_mlp_random_search(\n",
    "        train_data, \n",
    "        val_data, \n",
    "        num_features, \n",
    "        num_classes\n",
    "    )\n",
    "    \n",
    "    # Print best parameters and performance\n",
    "    print(\"\\nBest MLP Parameters:\")\n",
    "    for param, value in mlp_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(f\"\\nMLP - Training F1 Score: {mlp_train_f1:.4f}\")\n",
    "    print(f\"MLP - Validation F1 Score: {mlp_val_f1:.4f}\")\n",
    "    \n",
    "    # MLP models don't provide probability outputs in Spark\n",
    "    mlp_y_pred, mlp_y_true, _ = evaluate_model(\"MLP\", mlp_val_preds, num_classes, has_probability=False)\n",
    "else:\n",
    "    print(\"Skipping MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect model names and scores for models that were run\n",
    "model_names = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "\n",
    "if RUN_MODELS['logistic_regression']:\n",
    "    model_names.append(\"Logistic Regression\")\n",
    "    train_scores.append(lr_train_f1)\n",
    "    val_scores.append(lr_val_f1)\n",
    "    \n",
    "if RUN_MODELS['random_forest']:\n",
    "    model_names.append(\"Random Forest\")\n",
    "    train_scores.append(rf_train_f1)\n",
    "    val_scores.append(rf_val_f1)\n",
    "    \n",
    "if RUN_MODELS['gbdt']:\n",
    "    model_names.append(\"GBDT\")\n",
    "    train_scores.append(gbt_train_f1)\n",
    "    val_scores.append(gbt_val_f1)\n",
    "    \n",
    "if RUN_MODELS['mlp']:\n",
    "    model_names.append(\"MLP\")\n",
    "    train_scores.append(mlp_train_f1)\n",
    "    val_scores.append(mlp_val_f1)\n",
    "\n",
    "# Check if we have any models to compare\n",
    "if not model_names:\n",
    "    print(\"No models were run for comparison.\")\n",
    "else:\n",
    "    # Create a comparison DataFrame\n",
    "    model_comparison = pd.DataFrame({\n",
    "        'Model': model_names,\n",
    "        'Training F1': train_scores,\n",
    "        'Validation F1': val_scores,\n",
    "        'Difference (Train-Val)': [train - val for train, val in zip(train_scores, val_scores)]\n",
    "    })\n",
    "    \n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(model_comparison)\n",
    "    \n",
    "    # Plot model comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    ind = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(ind - width/2, train_scores, width, label='Training F1')\n",
    "    plt.bar(ind + width/2, val_scores, width, label='Validation F1')\n",
    "    \n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Model Comparison')\n",
    "    plt.xticks(ind, model_names, rotation=15)\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(train_scores):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "        \n",
    "    for i, v in enumerate(val_scores):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Set Evaluation with Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_names:  # Only run if we have models\n",
    "    # Find the best model based on validation F1 scores\n",
    "    best_model_index = val_scores.index(max(val_scores))\n",
    "    best_model_name = model_names[best_model_index]\n",
    "    print(f\"Best Model: {best_model_name} with Validation F1: {max(val_scores):.4f}\")\n",
    "    \n",
    "    # Get the corresponding model object\n",
    "    if best_model_name == \"Logistic Regression\" and RUN_MODELS['logistic_regression']:\n",
    "        best_model = lr_best_model\n",
    "        has_probability = True\n",
    "    elif best_model_name == \"Random Forest\" and RUN_MODELS['random_forest']:\n",
    "        best_model = rf_best_model\n",
    "        has_probability = True\n",
    "    elif best_model_name == \"GBDT\" and RUN_MODELS['gbdt']:\n",
    "        best_model = gbt_best_model\n",
    "        has_probability = False\n",
    "    elif best_model_name == \"MLP\" and RUN_MODELS['mlp']:\n",
    "        best_model = mlp_best_model\n",
    "        has_probability = False\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    test_predictions = best_model.transform(test_data)\n",
    "    \n",
    "    # Initialize the evaluator for F1 score\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='f1')\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_f1 = evaluator.evaluate(test_predictions)\n",
    "    print(f\"Test F1 Score with {best_model_name}: {test_f1:.4f}\")\n",
    "    \n",
    "    # Run comprehensive evaluation on test set\n",
    "    print(f\"\\n--- {best_model_name} Test Set Evaluation ---\")\n",
    "    test_y_pred, test_y_true, test_y_pred_proba = evaluate_model(\n",
    "        f\"{best_model_name} (Test)\", \n",
    "        test_predictions, \n",
    "        num_classes, \n",
    "        has_probability=has_probability\n",
    "    )\n",
    "else:\n",
    "    print(\"No models were run, skipping test set evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we've implemented a modular framework for training and evaluating multiple classification models using PySpark with random search hyperparameter optimization.\n",
    "\n",
    "The key features of this framework are:\n",
    "\n",
    "1. **Flexible Model Selection**: Users can easily select which models to run by setting configuration flags\n",
    "\n",
    "2. **Modular Design**:\n",
    "   - Random search functionality is contained in reusable functions\n",
    "   - Evaluation metrics and visualizations are handled by dedicated functions\n",
    "   - Each model follows the same workflow structure\n",
    "\n",
    "3. **Comprehensive Evaluation**:\n",
    "   - F1 scores for both training and validation data\n",
    "   - Confusion matrices\n",
    "   - Detailed classification reports \n",
    "   - ROC curves and AUC values (where applicable)\n",
    "   - Precision-Recall curves (where applicable)\n",
    "\n",
    "4. **Models Implemented**:\n",
    "   - Logistic Regression: Tuned for regularization, elastic net mixing, iterations, and model family\n",
    "   - Random Forest: Optimized for tree count, depth, impurity measure, and min instances per node\n",
    "   - Gradient Boosted Trees: Fine-tuned for iterations, depth, learning rate, and min instances per node\n",
    "   - Multilayer Perceptron: Explored different network architectures, block sizes, iterations, and learning rates\n",
    "\n",
    "This framework makes it easy to run experiments across different machines and with different team members by simply configuring which models to run at the beginning of the notebook."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
